#!/usr/bin/env python3
"""
LTX-2 ç”Ÿäº§çº§å®Œæ•´æµ‹è¯•è„šæœ¬
åŸºäº workflow_api_format.json çš„çœŸå®å·¥ä½œæµ
åŒ…å«æ‰€æœ‰èŠ‚ç‚¹ï¼šLoRAã€äººå£°æå–ã€å›¾ç‰‡é¢„å¤„ç†ã€å®Œæ•´ç”Ÿæˆæµç¨‹
"""
import json
import time
import random
from urllib import request
import os

BASE_URL = "http://localhost:8188"

def upload_file(filepath):
    """ä¸Šä¼ æ–‡ä»¶åˆ° ComfyUI"""
    filename = os.path.basename(filepath)
    print(f"ğŸ“¤ Uploading {filename}...")

    with open(filepath, 'rb') as f:
        data = f.read()

    boundary = 'Boundary' + ''.join(random.choices('0123456789', k=16))
    body = (
        f'--{boundary}\r\n'
        f'Content-Disposition: form-data; name="image"; filename="{filename}"\r\n'
        f'Content-Type: application/octet-stream\r\n\r\n'
    ).encode() + data + f'\r\n--{boundary}--\r\n'.encode()

    req = request.Request(
        f"{BASE_URL}/upload/image",
        data=body,
        headers={'Content-Type': f'multipart/form-data; boundary={boundary}'}
    )

    with request.urlopen(req, timeout=30) as resp:
        result = json.loads(resp.read())
        print(f"   âœ… {result.get('name')}")
        return result.get('name')

def check_comfyui():
    """æ£€æŸ¥ ComfyUI çŠ¶æ€"""
    print("ğŸ” Checking ComfyUI status...")
    try:
        req = request.Request(f"{BASE_URL}/system_stats")
        with request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read())
            print(f"   âœ… ComfyUI {data['system']['comfyui_version']}")
            print(f"   âœ… PyTorch {data['system']['pytorch_version']}")
            return True
    except Exception as e:
        print(f"   âŒ ComfyUI not responding: {e}")
        return False

def create_full_workflow(image_name, audio_name, seed=None):
    """
    åˆ›å»ºå®Œæ•´çš„ç”Ÿäº§çº§å·¥ä½œæµ
    åŸºäº workflow_api_format.json
    """
    if seed is None:
        seed = random.randint(0, 2**48)

    return {
        # ä¸»æ¨¡å‹åŠ è½½
        "184": {
            "inputs": {
                "ckpt_name": "ltx-2-19b-dev-fp8.safetensors"
            },
            "class_type": "CheckpointLoaderSimple"
        },
        # æ–‡æœ¬ç¼–ç å™¨åŠ è½½
        "155": {
            "inputs": {
                "t5_model": "gemma_3_12B_it_fp8_scaled.safetensors",
                "ckpt_name": "ltx-2-19b-dev-fp8.safetensors",
                "precision": "default"
            },
            "class_type": "LTXAVTextEncoderLoader"
        },
        # éŸ³é¢‘ VAE åŠ è½½
        "171": {
            "inputs": {
                "ckpt_name": "ltx-2-19b-dev-fp8.safetensors"
            },
            "class_type": "LTXVAudioVAELoader"
        },
        # MelBandRoformer äººå£°æå–æ¨¡å‹
        "272": {
            "inputs": {
                "model_name": "MelBandRoformer_fp16.safetensors"
            },
            "class_type": "MelBandRoFormerModelLoader"
        },
        # LoRA 1: Distilled
        "288": {
            "inputs": {
                "lora_name": "ltx-2-19b-distilled-lora-384.safetensors",
                "strength_model": 0.6,
                "model": ["184", 0]
            },
            "class_type": "LoraLoaderModelOnly"
        },
        # LoRA 2: Detailer
        "290": {
            "inputs": {
                "lora_name": "ltx-2-19b-ic-lora-detailer.safetensors",
                "strength_model": 1.0,
                "model": ["288", 0]
            },
            "class_type": "LoraLoaderModelOnly"
        },
        # LoRA 3: Camera Control
        "289": {
            "inputs": {
                "lora_name": "ltx-2-19b-lora-camera-control-dolly-in.safetensors",
                "strength_model": 0.5,
                "model": ["290", 0]
            },
            "class_type": "LoraLoaderModelOnly"
        },
        # åŠ è½½å›¾ç‰‡
        "240": {
            "inputs": {
                "image": image_name,
                "upload": "image"
            },
            "class_type": "LoadImage"
        },
        # åŠ è½½éŸ³é¢‘
        "243": {
            "inputs": {
                "audio": audio_name,
                "upload": "audio"
            },
            "class_type": "LoadAudio"
        },
        # è£å‰ªéŸ³é¢‘åˆ°10ç§’
        "244": {
            "inputs": {
                "max_duration": 10,
                "duration": 10,
                "audio": ["243", 0]
            },
            "class_type": "TrimAudioDuration"
        },
        # æå–äººå£°
        "271": {
            "inputs": {
                "model": ["272", 0],
                "audio": ["244", 0]
            },
            "class_type": "MelBandRoFormerSampler"
        },
        # è°ƒæ•´å›¾ç‰‡å°ºå¯¸
        "241": {
            "inputs": {
                "width": 768,
                "height": 512,
                "upscale_method": "lanczos",
                "crop": "pad_edge",
                "pad_fill": "0, 0, 0",
                "align": "center",
                "divisor": 2,
                "device": "cpu",
                "image": ["240", 0]
            },
            "class_type": "ImageResizeKJv2"
        },
        # LTX å›¾ç‰‡é¢„å¤„ç†
        "269": {
            "inputs": {
                "strength": 42,
                "image": ["241", 0]
            },
            "class_type": "LTXVPreprocess"
        },
        # æ­£å‘æç¤ºè¯
        "169": {
            "inputs": {
                "text": "A person speaks naturally with perfect lip synchronization to the audio. Expressive facial movements, natural eye contact with the camera. Professional studio lighting, cinematic quality, high detail.",
                "clip": ["155", 0]
            },
            "class_type": "CLIPTextEncode"
        },
        # è´Ÿå‘æç¤ºè¯
        "165": {
            "inputs": {
                "text": "static, bad teeth, deformed teeth, blurry, overexposed, underexposed, low contrast, artifacts, distorted, disfigured",
                "clip": ["155", 0]
            },
            "class_type": "CLIPTextEncode"
        },
        # ç©º latent è§†é¢‘
        "162": {
            "inputs": {
                "width": 768,
                "height": 512,
                "length": 297,
                "batch_size": 1
            },
            "class_type": "EmptyLTXVLatentVideo"
        },
        # å›¾ç‰‡è½¬è§†é¢‘ latent
        "239": {
            "inputs": {
                "start_frame": 1,
                "end_at_last": False,
                "vae": ["184", 2],
                "image": ["269", 0],
                "latent": ["162", 0]
            },
            "class_type": "LTXVImgToVideoInplace"
        },
        # éŸ³é¢‘ VAE ç¼–ç 
        "242": {
            "inputs": {
                "audio": ["271", 0],
                "audio_vae": ["171", 0]
            },
            "class_type": "LTXVAudioVAEEncode"
        },
        # åˆ›å»ºé®ç½©
        "249": {
            "inputs": {
                "value": 0,
                "width": 768,
                "height": 512
            },
            "class_type": "SolidMask"
        },
        # è®¾ç½® latent noise mask
        "248": {
            "inputs": {
                "samples": ["242", 0],
                "mask": ["249", 0]
            },
            "class_type": "SetLatentNoiseMask"
        },
        # è¿æ¥éŸ³è§†é¢‘ latent
        "166": {
            "inputs": {
                "video_latent": ["239", 0],
                "audio_latent": ["248", 0]
            },
            "class_type": "LTXVConcatAVLatent"
        },
        # LTX æ¡ä»¶åŒ–
        "164": {
            "inputs": {
                "frame_rate": 30,
                "positive": ["169", 0],
                "negative": ["165", 0]
            },
            "class_type": "LTXVConditioning"
        },
        # éšæœºå™ªå£°
        "178": {
            "inputs": {
                "noise_seed": seed
            },
            "class_type": "RandomNoise"
        },
        # é‡‡æ ·å™¨é€‰æ‹©
        "154": {
            "inputs": {
                "sampler_name": "euler"
            },
            "class_type": "KSamplerSelect"
        },
        # è°ƒåº¦å™¨
        "238": {
            "inputs": {
                "scheduler": "simple",
                "steps": 8,
                "denoise": 1.0,
                "model": ["289", 0]
            },
            "class_type": "BasicScheduler"
        },
        # CFG å¼•å¯¼
        "153": {
            "inputs": {
                "cfg": 1.0,
                "model": ["289", 0],
                "positive": ["164", 0],
                "negative": ["164", 1]
            },
            "class_type": "CFGGuider"
        },
        # é«˜çº§é‡‡æ ·å™¨
        "161": {
            "inputs": {
                "noise": ["178", 0],
                "guider": ["153", 0],
                "sampler": ["154", 0],
                "sigmas": ["238", 0],
                "latent_image": ["166", 0]
            },
            "class_type": "SamplerCustomAdvanced"
        },
        # åˆ†ç¦»éŸ³è§†é¢‘ latent
        "245": {
            "inputs": {
                "av_latent": ["161", 0]
            },
            "class_type": "LTXVSeparateAVLatent"
        },
        # VAE åˆ†å—è§£ç 
        "234": {
            "inputs": {
                "tile_size": 512,
                "overlap": 64,
                "temporal_size": 4096,
                "temporal_overlap": 8,
                "samples": ["245", 0],
                "vae": ["184", 2]
            },
            "class_type": "VAEDecodeTiled"
        },
        # è§†é¢‘åˆæˆè¾“å‡º
        "190": {
            "inputs": {
                "frame_rate": 30,
                "loop_count": 0,
                "filename_prefix": "ltx2_output",
                "format": "video/h264-mp4",
                "pix_fmt": "yuv420p",
                "crf": 19,
                "save_metadata": True,
                "trim_to_audio": True,
                "pingpong": False,
                "save_output": True,
                "images": ["234", 0],
                "audio": ["244", 0]
            },
            "class_type": "VHS_VideoCombine"
        }
    }

def main():
    print("=" * 70)
    print("LTX-2 Production Test - Full Workflow")
    print("Based on workflow_api_format.json")
    print("=" * 70)

    # æ£€æŸ¥ ComfyUI
    if not check_comfyui():
        return

    print("\nğŸ“ Step 1: Uploading files...")
    try:
        image_name = upload_file('/workspace/ltx_test/test_input.jpg')
        audio_name = upload_file('/workspace/ltx_test/test_audio.mp3')
    except Exception as e:
        print(f"âŒ Upload failed: {e}")
        return

    # åˆ›å»ºå®Œæ•´å·¥ä½œæµ
    print("\nğŸ”§ Step 2: Creating production workflow...")
    print("   ğŸ“¦ Loading: Checkpoint + 3 LoRAs")
    print("   ğŸµ Audio: MelBandRoformer vocal extraction")
    print("   ğŸ–¼ï¸  Image: Resize + LTX preprocess")
    print("   ğŸ¬ Generate: 297 frames @ 30fps (~10 seconds)")
    print("   ğŸ’¾ Output: H.264 MP4 with audio")

    workflow = create_full_workflow(image_name, audio_name)

    print("\nğŸš€ Step 3: Submitting to ComfyUI...")
    payload = {
        "prompt": workflow,
        "client_id": f"production_test_{int(time.time())}"
    }

    req = request.Request(
        f"{BASE_URL}/prompt",
        data=json.dumps(payload).encode(),
        headers={'Content-Type': 'application/json'}
    )

    try:
        with request.urlopen(req, timeout=30) as resp:
            response_data = resp.read()
            result = json.loads(response_data)

            if 'error' in result:
                print(f"âŒ Workflow error: {result['error']}")
                if 'node_errors' in result:
                    print(f"\nğŸ“‹ Node errors:")
                    for node_id, error in result['node_errors'].items():
                        print(f"   Node {node_id}: {error}")
                return

            prompt_id = result.get('prompt_id')
            print(f"âœ… Success! Prompt ID: {prompt_id}")
            print("\n" + "=" * 70)
            print("ğŸ¬ FULL PRODUCTION WORKFLOW STARTED!")
            print("=" * 70)
            print("\nâ±ï¸  Expected time: ~3-8 minutes (depends on GPU)")
            print("\nğŸ“Š Workflow includes:")
            print("   â€¢ Checkpoint: ltx-2-19b-dev-fp8.safetensors")
            print("   â€¢ LoRAs: Distilled (0.6) + Detailer (1.0) + Camera (0.5)")
            print("   â€¢ Audio: Vocal extraction with MelBandRoformer")
            print("   â€¢ Video: 768x512, 297 frames, 30fps")
            print("   â€¢ Quality: CRF 19 (high quality)")
            print("\nğŸ“Š Monitor progress:")
            print("   curl -s localhost:8188/queue | python3 -m json.tool")
            print("\nğŸ“ Output location:")
            print("   /workspace/ComfyUI/output/ltx2_output_*.mp4")
            print("\nğŸ” Check outputs:")
            print("   ls -lht /workspace/ComfyUI/output/ | head -5")
            print("\nğŸ’¡ Watch queue in real-time:")
            print("   watch -n 2 'curl -s localhost:8188/queue | python3 -c \"import sys,json; d=json.load(sys.stdin); print(f\\\"Running: {len(d.get(\\\\\\\"queue_running\\\\\\\",[]))}, Pending: {len(d.get(\\\\\\\"queue_pending\\\\\\\",[]))}\\\")'")
            print("=" * 70)
            return prompt_id

    except request.HTTPError as e:
        print(f"âŒ HTTP Error {e.code}: {e.reason}")
        error_body = e.read().decode('utf-8')
        print(f"\nğŸ“‹ Response body:")
        print(error_body)
        try:
            error_data = json.loads(error_body)
            if 'error' in error_data:
                print(f"\nâŒ Error detail: {error_data['error']}")
            if 'node_errors' in error_data:
                print(f"\nğŸ“‹ Node errors:")
                for node_id, error in error_data['node_errors'].items():
                    print(f"   Node {node_id}: {error}")
        except:
            pass
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
